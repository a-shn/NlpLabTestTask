# NlpLabTestTask
1. Для лемматизации нужен скрипт lemmatizer.py, в который в качестве аргументов подается архив с данными (в нашем случае covid_tweets.csv.gz) и путь с названием для .csv файла с лематизированными твитами:
python3 lemmatizer.py covid_tweets.csv.gz lemmatized_tweets.csv
Результатом скрипта является .csv-файл, содержащий одну колонку 'text',  котором содержаться лемматизированные твиты.

2.  1.) Чтобы удалить стоп-слова нужен скрипт stop_words_deleter.py, в который в качестве аргументов подается файл из предыдущего пункта и путь с названием для .csv файла с удаленными стоп-словами:
python3 stop_words_deleter.py lemmatized_tweets.csv tweets_without_stopwords.csv
Результатом скрипта также является .csv-файл, содержащий одну колонку 'text'.
  2.) Для удаления слов, которые встречаются менее 5 раз, нужен скрипт delete_less_occurred_words.py, в качестве аргументов подается файл из предыдущего шага и путь до нового .csv-файла:
python3 delete_less_occurred_words.py tweets_without_stopwords.csv clear_tweets.csv
Результатом скрипта является .csv-файл, содержащий одну колонку 'text'.
  3.) Для создания таблицы частот нужен скрипт counter.py, в который в качестве аргументов подается .csv-файл с одной колонкой 'text', в нашем случае это clear_tweets.csv:
python3 counter.py clear_tweets.csv words_occurrences.csv

3. Нужно запустить скрипт LDA_mallet.py и подать в качестве аргумента файл, прошедший пре-процессинг:
python3 LDA_mallet.py clear_tweets.csv
Результатом скрипта будет 3 файла: lda20.txt, lda30.txt, lda50.txt, которые будут содержать 20 наиболее вероятных слов для каждого топика.

4. Перед запуском нужно загрузить данные, в нашем случае это clear_tweets.csv, указать до него путь и последовательно запускать каждую "клетку". Результатом будет .html-файл с визуализацией.
